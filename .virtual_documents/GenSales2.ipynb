import pandas as pd
import numpy as np
from datetime import timedelta

# Constants
N_ITEMS = 5000
N_NOISE_ITEMS = 400
N_CLASSES = 200
N_DEPTS = 20
START_DATE = pd.to_datetime("today").normalize() - pd.DateOffset(years=4)
END_DATE = pd.to_datetime("today").normalize()
DATES = pd.date_range(start=START_DATE, end=END_DATE, freq='W-SAT')  # Weekly data, ending on Saturday



def generate_noise_items(n, classes, depts):
    noise_items = []
    for _ in range(n):
        season = np.random.choice(['BASIC', 'SPRING', 'FALL'])
        item_properties = define_item_seasonal_properties(season)
        item = {
            'Item': f'NOISE{np.random.randint(20, 100)}',
            'Class': np.random.choice(classes),
            'Dept': np.random.choice(depts),
            'Season': season,
            'Properties': item_properties
        }
        noise_items.append(item)
    return noise_items


# # Function to generate individual item properties for seasonal sales pattern
# def define_item_seasonal_properties():
#     return {
#         'start_week': np.random.randint(0, 8),  # Random start week within the first two months
#         'ramp_up_duration': np.random.randint(1, 3),  # 1-2 weeks of ramp up
#         'peak_duration': np.random.randint(8, 13),  # 8-12 weeks of peak sales
#         'sell_down_duration': np.random.randint(18, 22)  # 6 to 9 weeks of sell down
#     }
# Function to generate individual item properties for seasonal sales pattern
def define_item_seasonal_properties(season):
    if season == "SPRING":
        # Spring starts around the 9th week and ends around the 39th week (March to September)
        start_week = np.random.randint(9, 21)  # Start weeks adjusted for Spring
    elif season == "FALL":
        # Fall starts around the 40th week and can wrap around to the 5th week of the next year (October to January)
        if np.random.rand() > 0.5:
            start_week = np.random.randint(40, 52)  # Start weeks adjusted for Fall in the same year
        else:
            start_week = np.random.randint(0, 5)  # Start weeks adjusted for Fall wrapping to the next year
    else:
        start_week = np.random.randint(0, 52)  # For BASIC or other unspecified seasons

    return {
        'start_week': start_week,
        'ramp_up_duration': np.random.randint(1, 3),  # 1-2 weeks of ramp up
        'peak_duration': np.random.randint(8, 13),  # 8-12 weeks of peak sales
        'sell_down_duration': np.random.randint(18, 22)  # approximately 18 to 22 weeks of sell down
    }

def generate_sales(item, dates):
    total_weeks = len(dates)
    sales = np.zeros(total_weeks, dtype=float)
    
    if item['Season'] in ['SPRING', 'FALL']:
        weeks_per_year = total_weeks // 4
        for year in range(4):
            year_start = year * weeks_per_year
            year_end = year_start + weeks_per_year
            
            # Using the defined properties
            start_week = year_start + item['Properties']['start_week']
            intro_duration = item['Properties']['ramp_up_duration']
            growth_duration = item['Properties']['peak_duration']
            decline_duration = item['Properties']['sell_down_duration']
            
            base_sales = np.random.randint(50, 150)
            peak_sales = np.random.randint(450, 1500)
            
            intro_growth = np.linspace(base_sales, peak_sales, intro_duration)
            peak_phase = np.full(growth_duration, peak_sales)
            decline_phase = np.linspace(peak_sales, base_sales, decline_duration)
            
            # Calculate the actual end indices considering the bounds of the year
            end_intro = min(start_week + intro_duration, year_end)
            end_peak = min(end_intro + growth_duration, year_end)
            end_decline = min(end_peak + decline_duration, year_end)
            
            # Assign phases to the sales array ensuring not to overflow the year's limit
            sales[start_week:end_intro] = intro_growth[:end_intro - start_week]
            sales[end_intro:end_peak] = peak_phase[:end_peak - end_intro]
            sales[end_peak:end_decline] = decline_phase[:end_decline - end_peak]

    elif item['Season'] == 'BASIC':
        base_sales = np.random.randint(200, 1000)
        fluctuations = np.random.normal(0, base_sales * 0.1, total_weeks)
        sales[:] = base_sales + fluctuations

    sales += np.random.normal(0, 50, total_weeks)
    apply_christmas_bump(sales, dates)

    return np.clip(sales, 50, None).round().astype(int)

def apply_christmas_bump(sales, dates):
    christmas_period = (dates.month == 12) & ((dates.day > 10) & (dates.day < 26))
    sales[christmas_period] *= np.random.uniform(1.5, 3.0, size=np.sum(christmas_period))



# Generate items
item_codes = ['STYCOL{0:06d}'.format(i) for i in range(1, N_ITEMS + 1)]
classes = ['C{0:06d}'.format(i) for i in np.random.randint(1, N_CLASSES + 1, N_ITEMS)]
departments = ['D{0:06d}'.format(i) for i in np.random.randint(1, N_DEPTS + 1, N_ITEMS)]
seasons = np.random.choice(['BASIC', 'SPRING', 'FALL'], size=N_ITEMS, p=[0.5, 0.25, 0.25])
properties = [define_item_seasonal_properties(season) for season in seasons]

items = pd.DataFrame({
    'Item': item_codes,
    'Class': classes,
    'Dept': departments,
    'Season': seasons,
    'Properties': properties
})

# Add noise items
noise_items = generate_noise_items(N_NOISE_ITEMS, classes, departments)
noise_items_df = pd.DataFrame(noise_items)
items = pd.concat([items, noise_items_df], ignore_index=True)

# Ensure each department follows similar seasonal patterns
dept_season_mapping = {f'D{d:06d}': np.random.choice(['BASIC', 'SPRING', 'FALL']) for d in range(1, N_DEPTS + 1)}
items['Season'] = items['Dept'].map(dept_season_mapping)

# Loop through each row in the items DataFrame to generate sales data
all_sales_data = pd.DataFrame()
for index, row in items.iterrows():
    sales_data = generate_sales(row, DATES)
    item_sales_df = pd.DataFrame({
        'Date': DATES,
        'Item': row['Item'],
        'Sales': sales_data,
        'Class': row['Class'],
        'Dept': row['Dept'],
        'Season': row['Season']
    })
    all_sales_data = pd.concat([all_sales_data, item_sales_df], ignore_index=True)

# Time hierarchy data features
all_sales_data['woy'] = all_sales_data['Date'].dt.isocalendar().week.astype('int32')
all_sales_data['moy'] = all_sales_data['Date'].dt.month.astype('int32')
all_sales_data['qoy'] = all_sales_data['Date'].dt.quarter.astype('int32')
all_sales_data['soy'] = all_sales_data['Date'].dt.month.map(lambda x: 1 if x <= 6 else 2).astype('int32')
all_sales_data['year'] = all_sales_data['Date'].dt.year.astype('int32')

# Save to CSV
all_sales_data.to_csv('department_store_sales.csv', index=False)
print("Dataset generated and saved.")


import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots

# Load the data
df = pd.read_csv('/home/py/data/LGBM_ONLINE_v1/data/raw/generated_sales.csv')

# Basic statistics
print("Dataset Overview:")
print(df.describe())

# Time-based analysis
df['Date'] = pd.to_datetime(df['Date'])
df['Year'] = df['Date'].dt.year
df['Month'] = df['Date'].dt.month
df['Day'] = df['Date'].dt.day

# Monthly sales trend
monthly_sales = df.groupby(['Year', 'Month'])['Sales'].sum().reset_index()
fig = px.line(monthly_sales, x='Month', y='Sales', color='Year', title='Monthly Sales Trend')
fig.show()

# Seasonal decomposition
from statsmodels.tsa.seasonal import seasonal_decompose
sales_ts = df.groupby('Date')['Sales'].sum()
result = seasonal_decompose(sales_ts, model='additive', period=52)
fig = make_subplots(rows=4, cols=1, subplot_titles=('Observed', 'Trend', 'Seasonal', 'Residual'))
fig.add_trace(go.Scatter(x=result.observed.index, y=result.observed, mode='lines'), row=1, col=1)
fig.add_trace(go.Scatter(x=result.trend.index, y=result.trend, mode='lines'), row=2, col=1)
fig.add_trace(go.Scatter(x=result.seasonal.index, y=result.seasonal, mode='lines'), row=3, col=1)
fig.add_trace(go.Scatter(x=result.resid.index, y=result.resid, mode='lines'), row=4, col=1)
fig.update_layout(height=900, title_text="Seasonal Decomposition of Sales")
fig.show()

# Filtered Top 10 items by sales (excluding noise items)
non_noise_df = df[~df['Item'].str.startswith('NOISE')]
top_items = non_noise_df.groupby('Item')['Sales'].sum().sort_values(ascending=False).head(10)
print(f'top 10 items list:\n{top_items.index.to_list()}')
fig = px.bar(
    top_items,
    x=top_items.index,
    y='Sales',
    title='Top 10 Non-Noise Items by Sales',
    labels={'x': 'Item Code', 'y': 'Total Sales'},
    color=top_items.values,
    color_continuous_scale='Bluered'
)
fig.update_layout(hovermode='x unified')
fig.show()

# Sales distribution by department
dept_sales = df.groupby('Dept')['Sales'].sum().sort_values(ascending=False)
fig = px.pie(dept_sales, values='Sales', names=dept_sales.index, title='Sales Distribution by Department')
fig.show()

# Heatmap of sales by day of week and month
df['DayOfWeek'] = df['Date'].dt.dayofweek
heatmap_data = df.pivot_table(values='Sales', index='DayOfWeek', columns='Month', aggfunc='mean')
fig = px.imshow(heatmap_data, title='Average Sales by Day of Week and Month')
fig.show()

# Box plot of sales by season
fig = px.box(df, x='Season', y='Sales', title='Sales Distribution by Season')
fig.show()

# Correlation matrix
corr_matrix = df[['Sales', 'woy', 'moy', 'qoy', 'soy', 'year']].corr()
fig = px.imshow(corr_matrix, title='Correlation Matrix of Sales and Time Features')
fig.show()

# Year-over-year growth
yoy_growth = df.groupby('Year')['Sales'].sum().pct_change() * 100
print("\nYear-over-Year Sales Growth:")
print(yoy_growth)

# Item lifecycle analysis
item_lifecycle = df.groupby('Item').agg({
    'Date': ['min', 'max'],
    'Sales': 'sum'
}).reset_index()
item_lifecycle.columns = ['Item', 'First_Sale', 'Last_Sale', 'Total_Sales']
item_lifecycle['Lifecycle_Days'] = (item_lifecycle['Last_Sale'] - item_lifecycle['First_Sale']).dt.days
print("\nItem Lifecycle Summary:")
print(item_lifecycle.describe())

# Save all figures
plt.close('all')




