### Geenerate a summary table, agg on time, for all items


import pandas as pd

# Load the CSV file
data_file = "/home/py/data/PREDICT_SALES_v1/data/processed/merged.csv"
df = pd.read_csv(data_file)

# Rename columns to match your requirements
df.rename(columns={
    'predict_cluster_kmeans_fs_sales': 'kmeans',
    'predict_cluster_volume_fs_sales': 'volume',
    'predict_cluster_seasonal_fs_sales': 'seasonal',
    'predict_cluster_hac_s_sales': 'hac',
    'predict_all_sales': 'all'
}, inplace=True)

# Specify your start week here
start_week = 2839  # for example

# Filter the DataFrame for rows where epoch_week is greater or equal to start_week
filtered_df = df[df['epoch_week'] >= start_week]

# Define the columns to sum
columns_to_sum = ['sales', 'ly', 'kmeans', 'volume', 'seasonal', 'hac', 'all', 'yhat']

# Sum the columns for each item_id
sum_df = filtered_df.groupby('item_id')[columns_to_sum].sum().reset_index()

# Calculate absolute differences and find closest value
model_cols = ['kmeans', 'volume', 'seasonal', 'hac', 'all']
columns_to_sum = ['sales', 'ly', 'kmeans', 'volume', 'seasonal', 'hac', 'all', 'yhat', 'lgb']
# sum_df['lgb'] = sum_df[model_cols].sub(sum_df['sales'], axis=0).abs().idxmin(axis=1).apply(lambda x: sum_df.loc[sum_df.name, x], axis=1)
closest_columns = sum_df[model_cols].sub(sum_df['sales'], axis=0).abs().idxmin(axis=1)

# Extract values from closest columns using list comprehension
sum_df['lgb'] = [sum_df.loc[idx, col] for idx, col in closest_columns.items()]
# Convert the 'lgb' column to integer type
sum_df['lgb'] = sum_df['lgb'].astype(int)

# Calculate the differences
for col in columns_to_sum[1:]:  # skip 'sales' since it's the base for subtraction
    sum_df[f'sales_diff_{col}'] = sum_df['sales'] - sum_df[col]

# # Rename columns to match your requirements
# sum_df.rename(columns={
#     'predict_cluster_kmeans_fs_sales': 'sum_kmeans',
#     'predict_cluster_volume_fs_sales': 'sum_volume',
#     'predict_cluster_seasonal_fs_sales': 'sum_seasonal',
#     'predict_cluster_hac_s_sales': 'sum_hac',
#     'predict_all_sales': 'sum_all',
#     'yhat': 'sum_yhat'
# }, inplace=True)
# 'item_id', 'sales', 'ly', 'kmeans', 'volume', 'seasonal', 'hac', 'all',
#        'yhat', 'sales_diff_ly', 'sales_diff_kmeans', 'sales_diff_volume',
#        'sales_diff_season
# Select and reorder columns as necessary
final_columns = [
    'item_id', 'sales', 'ly', 'lgb',
    'yhat', 'sales_diff_ly', 'sales_diff_lgb','sales_diff_yhat'
]
final_df = sum_df[final_columns]

# Display the final DataFrame
print(final_df.head())
data_file = "/home/py/data/PREDICT_SALES_v1/data/processed/summary_table.csv"
final_df.to_csv(data_file, index=False)


print(sum_df.columns)





import pandas as pd
import numpy as np

# Example data structure (columns: Item, Sales, LY, Prophet, LightGBM, Time)
data_file = "/home/py/data/PREDICT_SALES_v1/data/processed/summary_table.csv"
df = pd.read_csv(data_file)

# Summary statistics for each item
item_stats = df.groupby("item_id").agg({
    "sales": ["mean", "std", "min", "max"],
    "ly": lambda x: np.sqrt(np.mean((x - df["sales"])**2)),  # RMSE for LY model
    "yhat": lambda x: np.mean(np.abs((x - df["sales"])/df["sales"]))*100,  # MAPE
    "lgb": lambda x: np.mean(x - df["sales"])  # Mean Bias
}).round(2)

print(item_stats)



