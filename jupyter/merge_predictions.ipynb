{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "218d0ce3-ffb1-4322-bd53-2df2557c42f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5345b7f7-c852-4392-b082-6cdde5f1e11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#----------------------------------------------------------------\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the CSV files into DataFrames\n",
    "data_file = \"/home/py/data/PREDICT_SALES_v1/data/processed/lgbm_predictions.csv\"\n",
    "df_lightgbm = pd.read_csv(data_file)\n",
    "\n",
    "data_file = \"/home/py/data/PREDICT_SALES_v1/data/processed/prophet_predictions.csv\"\n",
    "df_prophet = pd.read_csv(data_file)\n",
    "\n",
    "# Make sure the date columns are in the correct datetime format\n",
    "df_prophet['ds'] = pd.to_datetime(df_prophet['ds'])\n",
    "df_lightgbm['date'] = pd.to_datetime(df_lightgbm['date'])\n",
    "\n",
    "# Rename the 'ds' column in df_prophet to 'date' to match df_lightgbm\n",
    "df_prophet.rename(columns={'ds': 'date'}, inplace=True)\n",
    "\n",
    "# Merge the DataFrames\n",
    "df_merged = pd.merge(df_lightgbm, df_prophet, on=['item_id', 'date'], how='left')\n",
    "\n",
    "# create lgbm column that has the best predictor\n",
    "# Define the predictor columns\n",
    "predictor_cols = [\n",
    "    'predict_cluster_volume_fs_sales',\n",
    "    'predict_cluster_hac_s_sales',\n",
    "    'predict_cluster_kmeans_fs_sales',\n",
    "    'predict_cluster_seasonal_fs_sales',\n",
    "    'predict_all_sales'\n",
    "]\n",
    "\n",
    "# Dictionary to store best predictor per item_id based on MAE\n",
    "best_predictors = {}\n",
    "\n",
    "# Group by item_id and determine best predictor\n",
    "for item_id, group in df_merged.groupby('item_id'):\n",
    "    mae_scores = {\n",
    "        col: np.mean(np.abs(group[col] - group['sales']))\n",
    "        for col in predictor_cols\n",
    "    }\n",
    "    best_predictor = min(mae_scores, key=mae_scores.get)\n",
    "    best_predictors[item_id] = best_predictor\n",
    "\n",
    "# Create 'lgbm' column by selecting the best predictor per item_id\n",
    "df_merged['lgbm'] = df_merged.apply(lambda row: row[best_predictors[row['item_id']]], axis=1)\n",
    "\n",
    "\n",
    "# Save the merged DataFrame to a new CSV file\n",
    "data_file = \"/home/py/data/PREDICT_SALES_v1/data/processed/merged.csv\"\n",
    "df_merged.to_csv(data_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48073639-c6c6-4988-86fe-b6e9f530563d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['date', 'item_id', 'sales', 'epoch_week', 'woy', 'moy', 'qoy', 'soy', 'year', 'epoch_week_sin', 'epoch_week_cos', 'copy_sales', 'ly', 'partition', 'lag_1', 'lag_4', 'lag_13', 'lag_52', 'build_4', 'build_13', 'build_52', 'roll_mean_4', 'roll_median_4', 'roll_std_4', 'roll_mean_13', 'roll_median_13', 'roll_std_13', 'roll_mean_52', 'roll_median_52', 'roll_std_52', 'is_high_outlier', 'item_id_encoded', 'life_to_date', 'holiday_flag', 'cluster_kmeans_fs', 'cluster_gaussian_f', 'cluster_hac_s', 'cluster_seasonal_fs', 'cluster_trend_f', 'cluster_volume_fs', 'predict_avg_sales', 'roll_max_4', 'roll_min_4', 'roll_max_13', 'roll_min_13', 'roll_max_52', 'roll_min_52', 'predict_cluster_volume_fs_sales', 'predict_cluster_hac_s_sales', 'predict_cluster_kmeans_fs_sales', 'predict_cluster_seasonal_fs_sales', 'predict_all_sales', 'yhat', 'lgbm']\n"
     ]
    }
   ],
   "source": [
    "print(df_merged.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4310c9ff-a197-4a6d-bcd1-ef116fc88757",
   "metadata": {},
   "source": [
    "### MAE summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f8325a7f-6887-4068-88c6-9332c4f0d709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items best predicted by each model:\n",
      "best_model\n",
      "lgbm    5051\n",
      "ly       998\n",
      "yhat     327\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1345566/88417915.py:22: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  item_mae_summary = filtered_df.groupby('item_id').apply(compute_mae).reset_index()\n"
     ]
    }
   ],
   "source": [
    "def compute_mae(group):\n",
    "    mae_ly = np.mean(np.abs(group['sales'] - group['ly']))\n",
    "    mae_yhat = np.mean(np.abs(group['sales'] - group['yhat']))\n",
    "    mae_lgbm = np.mean(np.abs(group['sales'] - group['lgbm']))\n",
    "    \n",
    "    # Find the model with the lowest MAE\n",
    "    best_model = min(\n",
    "        [('ly', mae_ly), ('yhat', mae_yhat), ('lgbm', mae_lgbm)],\n",
    "        key=lambda x: x[1]\n",
    "    )[0]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'mae_ly': mae_ly,\n",
    "        'mae_yhat': mae_yhat,\n",
    "        'mae_lgbm': mae_lgbm,\n",
    "        'best_model': best_model\n",
    "    })\n",
    "\n",
    "filtered_df = df_merged[(df_merged['epoch_week'] >= 2843) & (df_merged['epoch_week'] <= 2883)]\n",
    "\n",
    "# Step 2: Group by item_id and apply the function\n",
    "item_mae_summary = filtered_df.groupby('item_id').apply(compute_mae).reset_index()\n",
    "\n",
    "# Step 3: Count how many items had each model as the best\n",
    "model_counts = item_mae_summary['best_model'].value_counts()\n",
    "\n",
    "# Output\n",
    "print(\"Number of items best predicted by each model:\")\n",
    "print(model_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21f54304-4681-4e7f-9a22-ab35cc17e250",
   "metadata": {},
   "source": [
    "### sum comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15706f10-5860-447a-8068-c76d781066d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of items best predicted by each model (by total error):\n",
      "best_model_by_sum\n",
      "lgbm    4068\n",
      "ly      1217\n",
      "yhat    1091\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1345566/4240500362.py:27: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  item_sum_comparison = filtered_df.groupby('item_id').apply(compute_total_error).reset_index()\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Group by item_id and calculate the total absolute error per model\n",
    "# epoch_test_start = 2843\n",
    "# epoch_test_end = 2883\n",
    "\n",
    "def compute_total_error(group):\n",
    "    sum_sales = group['sales'].sum()\n",
    "    \n",
    "    total_error_ly = abs(sum_sales - group['ly'].sum())\n",
    "    total_error_yhat = abs(sum_sales - group['yhat'].sum())\n",
    "    total_error_lgbm = abs(sum_sales - group['lgbm'].sum())\n",
    "    \n",
    "    # Find the model with the lowest total error\n",
    "    best_model_by_sum = min(\n",
    "        [('ly', total_error_ly), ('yhat', total_error_yhat), ('lgbm', total_error_lgbm)],\n",
    "        key=lambda x: x[1]\n",
    "    )[0]\n",
    "    \n",
    "    return pd.Series({\n",
    "        'sum_sales': sum_sales,\n",
    "        'total_error_ly': total_error_ly,\n",
    "        'total_error_yhat': total_error_yhat,\n",
    "        'total_error_lgbm': total_error_lgbm,\n",
    "        'best_model_by_sum': best_model_by_sum\n",
    "    })\n",
    "\n",
    "# Step 2: Apply the function to each item\n",
    "item_sum_comparison = filtered_df.groupby('item_id').apply(compute_total_error).reset_index()\n",
    "\n",
    "# Step 3: Count how many times each model had the lowest total error\n",
    "best_by_sum_counts = item_sum_comparison['best_model_by_sum'].value_counts()\n",
    "\n",
    "# Output\n",
    "print(\"Number of items best predicted by each model (by total error):\")\n",
    "print(best_by_sum_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "953656cf-d3e9-47a0-8f12-5c645a8e4112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Min epoch:2790  Max epoch: 2883\n"
     ]
    }
   ],
   "source": [
    "print(f\"Min epoch:{df_merged['epoch_week'].min()}  Max epoch: {df_merged['epoch_week'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7b5acb1-3b59-44dd-a3f8-ea63e8053fd0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<bound method IndexOpsMixin.tolist of Index(['date', 'item_id', 'sales', 'epoch_week', 'woy', 'moy', 'qoy', 'soy',\n",
      "       'year', 'epoch_week_sin', 'epoch_week_cos', 'copy_sales', 'ly',\n",
      "       'partition', 'lag_1', 'lag_4', 'lag_13', 'lag_52', 'build_4',\n",
      "       'build_13', 'build_52', 'roll_mean_4', 'roll_median_4', 'roll_std_4',\n",
      "       'roll_mean_13', 'roll_median_13', 'roll_std_13', 'roll_mean_52',\n",
      "       'roll_median_52', 'roll_std_52', 'is_high_outlier', 'item_id_encoded',\n",
      "       'life_to_date', 'holiday_flag', 'cluster_kmeans_fs',\n",
      "       'cluster_gaussian_f', 'cluster_hac_s', 'cluster_seasonal_fs',\n",
      "       'cluster_trend_f', 'cluster_volume_fs', 'predict_avg_sales',\n",
      "       'roll_max_4', 'roll_min_4', 'roll_max_13', 'roll_min_13', 'roll_max_52',\n",
      "       'roll_min_52', 'predict_cluster_volume_fs_sales',\n",
      "       'predict_cluster_hac_s_sales', 'predict_cluster_kmeans_fs_sales',\n",
      "       'predict_cluster_seasonal_fs_sales', 'predict_all_sales', 'yhat',\n",
      "       'lgbm'],\n",
      "      dtype='object')>\n"
     ]
    }
   ],
   "source": [
    "print(df_merged.columns.tolist)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26bc6a0e-76e0-4b5c-a2d0-a96bbc2d0c3c",
   "metadata": {},
   "source": [
    "### overall predictors units off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ce71f77-bada-445c-bda9-fde54055600a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total units sold: 120,872,614\n",
      "LY prediction error: 33,611,028 units\n",
      "YHAT prediction error: 33,725,201 units\n",
      "LGBM prediction error: 24,883,572 units\n",
      "Best model by total units: lgbm (off by 24,883,572 units)\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Sum actual and predicted sales\n",
    "sum_sales = filtered_df['sales'].sum()\n",
    "sum_ly = filtered_df['ly'].sum()\n",
    "sum_yhat = filtered_df['yhat'].sum()\n",
    "sum_lgbm = filtered_df['lgbm'].sum()\n",
    "\n",
    "# Step 3: Compute total unit error for each predictor\n",
    "error_ly = abs(sum_sales - sum_ly)\n",
    "error_yhat = abs(sum_sales - sum_yhat)\n",
    "error_lgbm = abs(sum_sales - sum_lgbm)\n",
    "\n",
    "# Step 4: Print results\n",
    "print(f\"Total units sold: {sum_sales:,.0f}\")\n",
    "print(f\"LY prediction error: {error_ly:,.0f} units\")\n",
    "print(f\"YHAT prediction error: {error_yhat:,.0f} units\")\n",
    "print(f\"LGBM prediction error: {error_lgbm:,.0f} units\")\n",
    "\n",
    "# Optional: Which model was closest overall\n",
    "best_model = min(\n",
    "    [('ly', error_ly), ('yhat', error_yhat), ('lgbm', error_lgbm)],\n",
    "    key=lambda x: x[1]\n",
    ")\n",
    "print(f\"Best model by total units: {best_model[0]} (off by {best_model[1]:,.0f} units)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab2f883-bd59-4551-8a3b-73f8b930e30e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml1 env",
   "language": "python",
   "name": "ml1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
